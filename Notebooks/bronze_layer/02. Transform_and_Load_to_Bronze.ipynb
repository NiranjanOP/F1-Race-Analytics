{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2549ec4f-c61b-4a4a-8e5c-970db65c6c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# ### 02. Transform and Load Bronze Table\n",
    "# In this notebook:\n",
    "# - Load raw FastF1 lap data (from Day 1 cache)\n",
    "# - Convert Pandas DataFrame → Spark DataFrame\n",
    "# - Handle unsupported dtypes (timedelta64)\n",
    "# - Write to Delta Bronze table in f1analytics.bronze schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc699779-b4e6-475a-994b-60f0f7ff306d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# #### 1. Import libraries and reload FastF1 session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a081001f-d50c-40d6-90d4-d078fa80e7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install fastf1 matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61e7064-dcdf-4680-bf27-3779fe012b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644c12df-f37b-4c63-b87b-c8d51fd22469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Enable cache (same as Day 1)\n",
    "fastf1.Cache.enable_cache(\"/Workspace/Users/niranjan.482000@gmail.com/F1-Race-Analytics/cache\")\n",
    "\n",
    "# Load one session (Monza GP 2023 - Race as example)\n",
    "session = fastf1.get_session(2023, \"Monza\", \"R\")\n",
    "session.load()\n",
    "\n",
    "# Get laps data\n",
    "df = session.laps\n",
    "print(\"Pandas DataFrame shape:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e708b3-1837-40b6-a233-86bca653e239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# #### 2. Convert Pandas → Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c676dc-e49a-4f1d-830d-be12af5841f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c1f89a-9dfb-49e5-a183-b327faf21fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## Convert timedeltas in pandas then create Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1f17a8-3275-40ee-b5f5-d7049daaa9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Convert timedelta columns to milliseconds in pandas\n",
    "timedelta_cols = [\n",
    "    \"Time\",\"LapTime\",\"PitOutTime\",\"PitInTime\",\n",
    "    \"Sector1Time\",\"Sector2Time\",\"Sector3Time\",\n",
    "    \"Sector1SessionTime\",\"Sector2SessionTime\",\"Sector3SessionTime\",\n",
    "    \"LapStartTime\"\n",
    "]\n",
    "\n",
    "# df is the pandas DataFrame from FastF1 (session.laps)\n",
    "import pandas as pd\n",
    "for c in timedelta_cols:\n",
    "    if c in df.columns:\n",
    "        # .dt may be NaT safe; result is float ms (NaN where missing)\n",
    "        df[c] = df[c].dt.total_seconds() * 1000.0\n",
    "\n",
    "# ensure datetime columns are proper dtype\n",
    "if \"LapStartDate\" in df.columns:\n",
    "    df[\"LapStartDate\"] = pd.to_datetime(df[\"LapStartDate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5044098-22d3-4912-9f98-8c033b6b2cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Create Spark DataFrame from pandas (now no INTERVAL types)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d732c9e9-83f4-4cf8-9703-df1d77a49299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## Rename to snake_case and cast ms -> long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca1628-7d87-4776-805b-41d8d26e0dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def to_snake(s: str) -> str:\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    return s2.lower()\n",
    "\n",
    "# rename columns\n",
    "for old in spark_df.columns:\n",
    "    spark_df = spark_df.withColumnRenamed(old, to_snake(old))\n",
    "\n",
    "# cast millisecond columns to long\n",
    "for c in timedelta_cols:\n",
    "    newc = to_snake(c)\n",
    "    if newc in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(newc, col(newc).cast(\"long\"))\n",
    "\n",
    "spark_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce71980-8d28-4bc2-b8f5-128b713dc5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## Write to bronze delta table and validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f61248-e824-4756-8608-bb43a7a0e915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"f1_catalog.bronze.lap_times\")\n",
    "\n",
    "# quick validation\n",
    "display(spark.sql(\"SELECT COUNT(*) AS cnt FROM f1_catalog.bronze.lap_times\"))\n",
    "display(spark.sql(\"SELECT driver, lap_number, lap_time, position, compound FROM f1_catalog.bronze.lap_times LIMIT 10\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6149911431129825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02. Transform_and_Load_to_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

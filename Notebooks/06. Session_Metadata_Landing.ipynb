{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3ae916-83d9-41bd-87cc-1a95b58dffe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install fastf1 matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b53b9d-8981-4994-8e8f-431397870fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39ec636-285f-441e-ba7d-f22152f87a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Robust session metadata ingestion -> writes to f1_catalog.bronze.session_metadata\n",
    "import fastf1, pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# optional: enable cache (ignore error if path differs)\n",
    "try:\n",
    "    fastf1.Cache.enable_cache(\"/Workspace/Users/niranjan.482000@gmail.com/F1-Race-Analytics/cache\")\n",
    "except Exception as e:\n",
    "    print(\"Cache enable warning:\", e)\n",
    "\n",
    "# load session (change year/gp/session as needed)\n",
    "year = 2023\n",
    "gp = \"Bahrain\"\n",
    "sess_type = \"R\"\n",
    "sess = fastf1.get_session(year, gp, sess_type)\n",
    "sess.load()\n",
    "\n",
    "# inspect event keys so you can see what's available\n",
    "try:\n",
    "    ev = dict(sess.event)\n",
    "except Exception:\n",
    "    ev = {}\n",
    "print(\"event keys:\", list(ev.keys()))\n",
    "\n",
    "# helper for safe extraction\n",
    "def safe_get(d, *keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "# build metadata with fallbacks\n",
    "metadata = {\n",
    "    \"session_key\": f\"{safe_get(ev,'EventName','Event','eventName', default=gp)}_{sess.name}_{pd.to_datetime(sess.date).strftime('%Y%m%d')}\",\n",
    "    \"year\": int(safe_get(ev,'EventYear','Year', default=year)),\n",
    "    \"round\": (int(safe_get(ev,'RoundNumber','Round', default=None)) if safe_get(ev,'RoundNumber','Round', default=None) is not None else None),\n",
    "    \"event_name\": safe_get(ev,'EventName','Event', default=gp),\n",
    "    \"country\": safe_get(ev,'Country','CountryName', default=None),\n",
    "    \"location\": safe_get(ev,'Location', default=None),\n",
    "    \"session_name\": sess.name,\n",
    "    \"session_type\": getattr(sess, 'session_type', None),\n",
    "    \"date\": pd.to_datetime(sess.date),\n",
    "    \"weather_temp_air\": (sess.weather_data['AirTemp'].mean() if getattr(sess, 'weather_data', None) is not None and 'AirTemp' in sess.weather_data.columns else None),\n",
    "    \"weather_temp_track\": (sess.weather_data['TrackTemp'].mean() if getattr(sess, 'weather_data', None) is not None and 'TrackTemp' in sess.weather_data.columns else None),\n",
    "    \"weather_humidity\": (sess.weather_data['Humidity'].mean() if getattr(sess, 'weather_data', None) is not None and 'Humidity' in sess.weather_data.columns else None),\n",
    "    \"weather_pressure\": (sess.weather_data['Pressure'].mean() if getattr(sess, 'weather_data', None) is not None and 'Pressure' in sess.weather_data.columns else None),\n",
    "}\n",
    "\n",
    "pdf = pd.DataFrame([metadata])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"session_key\", StringType(), False),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"round\", IntegerType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"session_name\", StringType(), True),\n",
    "    StructField(\"session_type\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"weather_temp_air\", DoubleType(), True),\n",
    "    StructField(\"weather_temp_track\", DoubleType(), True),\n",
    "    StructField(\"weather_humidity\", DoubleType(), True),\n",
    "    StructField(\"weather_pressure\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# create spark DF and write to catalog\n",
    "sdf = spark.createDataFrame(pdf, schema=schema)\n",
    "sdf.write.format(\"delta\").mode(\"append\").saveAsTable(\"f1_catalog.bronze.session_metadata\")\n",
    "\n",
    "print(\"Wrote session metadata to f1_catalog.bronze.session_metadata\")\n",
    "display(spark.table(\"f1_catalog.bronze.session_metadata\").orderBy(\"date\", ascending=False).limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06. Session_Metadata_Landing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
